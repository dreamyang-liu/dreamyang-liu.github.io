[{"title":"MLSys 学习笔记 1","url":"/2025/04/17/MLSys-杂记1/","content":"1. GPU ECC（Error‑Correcting Code） 原理：在显存（VRAM）中为每 N 位数据生成冗余校验位，常见为 SECDED（单错纠正、双错检测）。 实现： 专业级卡（Tesla/Quadro/A100、AMD Instinct）默认支持 ECC，消费级卡多关闭或不支持。 ECC 逻辑集成在内存控制器，对程序透明。 优缺点： 优点：避免显存位翻转导致的 Silent Data Corruption，可监控纠错/未纠错计数。 缺点：约 12.5% 显存开销、5–12% 带宽与延迟损失。 开启与监控（NVIDIA 举例）: 2. NVIDIA Persistence Mode（持久化模式） 作用：驱动在无客户端时依然保持 GPU 上下文初始化，减少短作业启动延迟；保持稳定 P‑state。 命令： 注意：功耗略增；可配合 nvidia-persistenced 守护进程跨重启生效。 Node 0: [ 1, 2, 3, 4]Node 1: [ 5, 6, 7, 8]Node 2: [ 9, 10, 11, 12]Node 3: [13, 14, 15, 16]","date":"2025-04-17","tags":["GPU","ECC","持久化模式","Ring Reduce","Reduce-Scatter"]},{"title":"Parentheses Problems","url":"/2023/09/17/Parentheses-Problems/","content":"Parentheses is a kind of interesting problem, it is mainly related to valid parentheses like: Check whether the string is a valid parentheses Find the longest valid parentheses (substring) in a substring Find the longest valid parentheses (subsequence) in a substring Generate all valid parentheses for given length … Before dive into each topic, let’s define what is a valid parentheses. (), ()()()() and ((((((()))))))(())().","date":"2023-09-17","categories":["Algorithm"],"tags":["String"]},{"title":"RateLimiter Algorithm","url":"/2023/09/09/RateLimiter/","content":"RateLimiter is a curcial component in a distributed system. Normally we can used it to avoid DDoS or malicious request.In this post, we are going the introduce several algorithm that can be used to do Rate Limiting. Token BucketA token bucket is a container that has pre-defined capacity. Tokens are put in the bucket at preset rates periodically. Once the bucket is full, no more tokens are added. As shown in Figure, the token bucket capacity is 4. The refiller puts 2 tokens into the bucket every second. Once the bucket is full, extra tokens will overflow. Token Bucket can handle burst request and can make sure the limit won’t exceed the preset capacity. Leaking BucketLeaking Bucket is like Token Bucket but it acts like a FIFO queue. The leaking bucket has a process rate which indicate how many requestsit can processs per seconds. And when a request come, it will first check whether the queue is full or not, if not, add the request to the queue, otherwise drop the request.Request in the queue are pulled from the queue and processed at regulard intervals. One concern with Leaking Bucket is that it was processed in an FIFO way, if the queue was filled with old request, new request will be rate-limited.","date":"2023-09-09"},{"title":"Golang Tutorial 1","url":"/2023/09/08/Golang-Tutorial-1/","content":"","date":"2023-09-08"},{"title":"Rust Tutorial 1","url":"/2023/09/08/Rust-Tutorial-1/","content":"","date":"2023-09-08"},{"title":"Backtracking Template","url":"/2023/09/02/Backtracking-Template/","content":"BacktrackingBacktracking is normally used to search the solution space. Such solution space normally is combinatorial solution space which increase exponentially. A good example is the permutation problem. Given a number , print out all the permutations of numbers of . For this problem, , the expected output should be In this case, we need to use backtrack to help us tackling this problem. **Backtrack can save the checkpoint every time when we need to make a decision on which candidate to expand. ** Here is a template for Backtracking. Let’s do a partial dry run for the permutation problem. We pick from candidates and remove from it We pick from candidates and remove from it We pick from candidates and remove from it candidates is empty and we jump out from backtrack # [1,2,3] We add back to candidates and find there’s no other candidate. We add back to candidates and move to the next candidate 3 We pick from candidates and remove from it We pick from candidates and remove from it candidates is empty and we jump out from backtrack # [1,3,2] Now let’s give an example of permutation problem. Obviously, the time complexity is not linear, actually, it should be the same of the number of leaf node which is .","date":"2023-09-02","categories":["Algorithm"],"tags":["Backtracking"]},{"title":"LangChain","url":"/2023/08/20/LangChain/","content":"LangChain is a framework that","date":"2023-08-20","categories":["AI"],"tags":["LLM","AI"]},{"title":"Docker Memo","url":"/2023/08/15/Docker-Memo/","content":"DockerfileDockerfile is a text file that contains the definition of a container image, which can be used to launch a container. The defintion contains the command, base image, configurations etc. In this post, we will go through each component and discuss in what scenario we will use them. Let’s get started. Docker imageBefore we dive into each components, we may want to know what exactly a docker image is. Reference: structure_of_an_image An image is a read-only template with instructions for creating a Docker container. Often, an image is_based on another image, with some additional customization. –FROM Docker Official Docker image can contains a lot files, to dive deep into detail, let’s pull an image first. From the above output, we know that we pull python:alpine3.18 image from dockerhub and this image has 5 layers. Base Image &#x2F; Parent ImageIn most Dockerfiles, the first line you will see is FROM &#123;base image&#125; like FROM python:3.8. This line defines the base image of the image we are going to build. The base image may contains one or more layers. RUNRUN command can be use to execute some command like installing packages, downloading files etc. One thing need to mention is that each command will add a new layer to existing image. Combining multiple command into a single RUN command is a common trick to make image smaller and with fewer layers. CMDThe CMD describes the default container parameters or commands. The user can easily override the default command when you use this. ENTRYPOINTENTRYPOINT is also used to define the commands that will ALWAYS run, which means it cannot be override. ENTRYPOINT can be used to ensure that a specific command is always run when a container starts, even if the user tries to override it. COPYCOPY can be used to copy local files into container image, and that’s all. ADDADD command also allows copying files&#x2F;folders from host to image. It supports tarball extraction and remote URL support. Generally, ADD is more preferrable than COPY. Now let’s take an example of using ADD in Dockerfile. Assuming we are in a folder /tmp/app and it has the following files, At the same time we write the following command to our Dockerfile. The above commmand will copy the file /tmp/app/server.js(host path) to /tmp/server.js(container image path). One thing we notice that the first argument should be within the context, the context can be interpreted as the path where we execute the docker build command. VOLUMEVolume can be treated as a persistent data storage for docker container. The following command means any container launched from this image, the /var/www/html directory on the host machine will be mounted to the /var/www/html directory inside the container. This means that any changes made to the /var/www/html directory inside the container will be reflected on the host machine, and vice versa. DiveDive is a very useful tool to analyze the container image. It enable you to see each layers’ command and change that reflected on the file system. Reference OCI Image Manifest Specification","date":"2023-08-15","categories":["Cloud Computing"],"tags":["Container","Docker"]},{"title":"SQL Chapter 2 各种JOINS","url":"/2023/08/13/SQL-Chapter-2-各种JOINS/","content":"Chapter 2 各种JOINSInner join join with different database Self Join: compound join condition implicit join syntax Outer joins outer join with mutiple tables USING natural join — cross join Unions:","date":"2023-08-13","tags":["MYSQL"]},{"title":"SQL Chapter 1 intro/LIKE/LIMIT/REGEXP/IS NULL","url":"/2023/08/13/SQL-Chapter-1-intro-LIKE-LIMIT-REGEXP-IS-NULL/","content":"Chapter 1 intro&#x2F;LIKE&#x2F;LIMIT&#x2F;REGEXP&#x2F;IS NULLUSE sql_store; – use this database AND has a higher presidence than OR 括号里是and BETWEEN AND LIKE REGEXP IS NULL&#x2F; IS NOT NULL 新命名一列 &#x2F; ORDER BY LIMIT (可以skip）","date":"2023-08-13","tags":["MYSQL"]},{"title":"Linux System Overview","url":"/2023/08/08/Linux-System-Overview/","content":"Linux is probably the most widely used operation system in this world. Knowing the fundamental of Linux System could benfit developer in a lot of aspects. In this blog, we will explore some fundamental topics. File SystemYou can find a more detailed File system Hierarchy here. KernelEnvironment VariablePATHPATH is one of the most important environment variable. By default, it will have the value of When you type a command, it will search all the above folders and find whether there’s a file with same name can be found. The user need to have the permission to execute the file other wise it will raise a permission error zsh: permission denied: db.json. External Devices","date":"2023-08-08","categories":["Overview"],"tags":["Linux"]},{"title":"Random Algorithm","url":"/2023/08/08/Random-Algorithm/","content":"Weighed random()random() is an important function that will return a random number with same probability. Weighed random() requires us to generate random number with different weights. A naive approach could be we can create an array of elements. And we randomly set elements to , elements to , elements to and elements to .Then we can randomly pick an index from to , and reutrn the value on the picked index. However, the above method has a really obvious drawback. It will consume a lot memory and setting value will take a lot time as well. Instead of creating an array, can we compress the info there ? Ther answer is yes, let’s think about the prefix sum array of the weights array of size (0-indexed). $$w’k = \\sum_0^{k-1} w_i$$Then given an array, we have and $w’{n+1} = \\sum_0^{n} w_inD_i:[w’i, w’{i+1}],\\ i \\in {0,1 \\dots n}W’kkD_i\\frac{w_i}{\\sum_0^{n-1} w_j}$. Shuffle AlgorithmFisher–Yates shuffle Algorithm can be used to generate randomized permutation of array. The principal behind the algorithm is that we need to make sure each pair of index i.e need to have the same probablity of exchange. In the above implementation, for each , we randomly choose a that to conduct exchange. Note that we also include since it’s possible we don’t change it at all. In this way, we are able to cover all pairs , and as we iterate i from to , we will be able to cover all pairs.","date":"2023-08-08","categories":["Algorithm"],"tags":["Randomize"]},{"title":"Monotonic Stack/Queue","url":"/2023/08/06/Monotonic-Stack-Queue/","content":"","date":"2023-08-06","tags":["Data Structure"]},{"title":"Union Find","url":"/2023/08/06/Union-Find/","content":"","date":"2023-08-06","tags":["Data Structure"]},{"title":"Trie Tree","url":"/2023/08/06/Trie-Tree/","content":"","date":"2023-08-06","tags":["Data Structure"]},{"title":"Fenwick Tree","url":"/2023/08/06/Fenwick-Tree/","content":"Fenwick Tree, also known as binary indexed tree, is a data structure that supports single point updates and range queries. In most case, we can use it to query range sum, range product or other operation that satisify: Associative Law: Having inverse operation, we can infer y given and It is based on the assumption that any prefix can be decomposed into at most ranges, such that the information of these logn ranges is known. As we can see from the above figure, the right boundary of the range controlled by each position is . In Fenwick, is assigned to control a range of length . Here, is the number of the first 1 bit from the right, minus 1. For example, $12 = (1100)2k = 22^k = 4c{12}[12 - 4 + 1, 12]2^kx\\ & \\ (-x)$ .","date":"2023-08-06","categories":["Algorithm"],"tags":["Array","Data Structure","Bit Trick"]},{"title":"Segment Tree","url":"/2023/08/06/Segment-Tree/","content":"Segment trees are a data structure commonly used to maintain interval information. Segment trees can be implemented in time complexity to perform operations such as single point update, range update, and range query (sum of range, maximum of range, minimum of range).","date":"2023-08-06","categories":["Algorithm"],"tags":["Data Structure","Segment Tree"]},{"title":"Consistent Hashing","url":"/2023/08/06/Consistent-Hashing/","content":"Consistent Hashing is a technique that commonly used in distributed system to distribute data into different servers / services /data centers. It can make sure: data -> node mapping is calcluated using hash function, data will be sent to the first node that has hash value When adding/removing nodes, only partial data need to do remapping, most data won’t be affected. Compared to traditional hash mapping, consistent hashing can make a more balanced data distributing MethodologyHere is a brief digram of how consistent hashing works. Consistent hashing is trying do the following: Create a hashing ring and evenly distribute existing node on the ring. Calculate the hashing value of the data and send it to the first node which has greater hashing value. When S3 node get removed, only the scope from be remaaped to S2. The above consistent hashing seems look good, however, some servers may be overloaded where as some servers are idel all the time. In this case, we may observe that, a large percentage of hash values of datas fall into a range i.e which is a subdomain of whole hash ring. In the above diagram, most ids fall into the scope of IP2, which represents as an imbalanced distribution. To resolve such issue, we introduced virual node to more evenly distribute the workload. Instread of having one node to represent the physical server on the hash ring, we actually create multiple virtual node and distribute them across the hash ring domain. In this case, if the data fall into a subdomain, we can still map them into different physical servers. At the same time, if we add or remove node, affected scope will be much smaller than naive consistent hashing since each segment will be much smaller.","date":"2023-08-06","categories":["System Design"]},{"title":"System Design Fundamentals","url":"/2023/08/06/System-Design-Fundamentals/","content":"CAP TheoremThe CAP theorem, or Brewer’s theorem, states that a distributed database system can only guarantee two out of three characteristics: consistency, availability, and partition tolerance. The system prioritizes availability over consistency and can respond with possibly stale data. Partition Tolerance在CAP理论中,Partition Tolerance(分区容忍性)是指系统在发生网络分区故障的时候,仍然能够对外提供服务的能力。 所谓网络分区故障,是指系统中的节点被分成两部分,节点间的网络通信被阻断。在这种情况下,一个分布式系统可能会表现出下面两种行为: 无法对外提供服务 整个系统不可用,对用户不可见。 以牺牲一致性为代价继续服务 系统继续对用户可见,但返回的数据可能不一致。 Partition Tolerance表示系统能够承受网络分区的发生,继续对外服务而不会完全崩溃。根据CAP理论,一个系统不可能同时满足一致性(Consistency)、可用性(Availability)和分区容忍性,最多只能同时满足两个。 所以一个Partition Tolerance系统通常会牺牲数据一致性来保证服务可用性。比如允许读取到过期数据,或两个分区的数据产生冲突。这在许多大规模分布式系统中都是可接受的权衡。 一个典型的Partition Tolerance(分区容忍性)的例子是DNS系统。 DNS采用了分布式的树形结构,一个域名由多个DNS服务器共同提供解析服务。当网络发生分区时,可能出现如下情况: 某区域用户无法访问部分DNS服务器,但可以访问到另一部分服务器,仍能得到域名解析结果。 不同的DNS服务器返回了不同的解析结果(IP地址不一致),但用户还是能得到响应。 这就是DNS系统表现出来的Partition Tolerance性质。当网络分区发生时,DNS系统为了继续服务,承受返回不一致数据的结果,而没有选择完全停止响应。 Partition Tolerance的另一个例子是一些分布式缓存系统,如Memcached。当节点间失去联系时,不同分区的数据可能不一致,但每个分区内部仍能继续使用本地缓存,整个系统不会完全停止服务。 这些系统都采用了设计理念:“允许读取脏数据”或者“允许短暂不一致”来实现Partition Tolerance。因为对大多数应用来说,与整个系统完全不可用相比,读取到陈旧或不一致的数据仍是可以接受的。 Modern Distrbuted System ComponentsLoad BalancerLoad Balancer is the entry point of an web application. It will distribute incoming customer traffic to difference worker&#x2F;servers. It can help Avoid a single server to be overloaded Redirect request to healthy server when one server become unavailable. Load Balancer plays an important role in enabling system to scale horizontally. AWS provides the following load balancer services: Classic Load Balancer: mainly used to distribute traffic to EC2 instances Application Load Balancer: more flexible, choose this one by default. Network Load Balancer Gateway Load Balancer CDNContent Delivery Network(CDN) is a service provided by third-party for fast file delivery. By providing a single endpoint, CDN servie will automatically route to the closet server to fetch the static files. It is mainly used to serve static files like HTML, javascript file and images. It can also host some files like video, tarball etc. It can: Reduce the load of web server Accelerate the file retrievel by fetching file from closest server. There is a TTL(Time to live) need to set for CDN. If TTL is set to be a small value, it will need to update the cache from source very often, the charge of data transfer will increase. If TTL is set to be a large value, customer may get stale file. The TTL should be set according to the need of system. DNSThe Domain Name System (DNS) is a database that translates domain names into Internet Protocol (IP) addresses. DNS maps the name that people use to locate a website to the IP address that a computer uses to locate that website. Web TierWeb Tier mainly refers to Web Service. Web service mainly contains business logic. Like fetching the data from database and return to user or submit an order on behalf of user etc. Ideally, web tier should be stateless since it can make web tier eailer to scale horizontally. Data TierData Tier mainly refers to database or persistent storage. The “persistent” means even if the server shut down, the data will not be lost. On the contaray, some in-memory cache system will lose all data if server get shut down. As data becomes more and more complex, people developed various data base system to accomodate different use case. Relational Database NoSQL (Not only SQL) Giving an expressive decription of database here would be too ambitious, we will use a single post to discuss it in detail. Cache TierCacher Tier emerges as querying database is such an expensive operation. In the mean while, some request can also be expensive like running a model to give calculation. Thanks to the Principle of Locality, we know the tendency of a processor to access the same set of memory locations repetitively over a short period of time. It means that the same data could be used multiple times over a short period of time. And that makes cache tier useful in real production. Cache works as a middle-ware inside the system and it can be scaled idenpendently. It also brings a series of challenges like cache overheat, cache missing and data synchronization. Like data tier, we will use a single post to discuss it in detail. Message QueueSometimes we may think if there’s chain of service calling. How could these subsystem to scale independently. i.e If service A is trying to call service B, if service B is not available at the time when service A made the call, then service A must wait, otherwise we will lose the request from customer. We may want something that can abstract service’s input producer and outcome consumer. Here is what message queue come into play. Message queues serve as temporary storage services. Producers can publish messages to a MQ regardless of whether consumers are working or not. At the same time, consumers can consume messages from a MQ regardless of whether producers are working or not.","date":"2023-08-06","categories":["System Design"],"tags":["Overview"]},{"title":"Binary Tree Topics","url":"/2023/08/06/Binary-Tree-Topics/","content":"TraverseThere’re a lot intereting topics about the traverse of binary tree like the in-order traverse of BST is a sorted sequence. And we may also want to know how to conduct a level tarverse(using queue) and it’s the fundamental of doing Breadth First Search(BFS). We will discuss these topics as well when we go through each section below. Pre-order TraversePre-order traverse will visit root node -> left child -> right child. In-Order TraverseIn-order traverse will visit left child -> root node -> right child. The in-order traverse of BST will return a sorted array. Post-Order TraverseIn-order traverse will visit left child -> right node -> root child. To determine a binary tree, we need in-order sequence + pre-order &#x2F; post-order sequence. Methodology of solving Binary Tree ProblemsOne important observation is that when we take a look at the position of root node, from pre-order to post-order, it goes from the first to the last. Since we always visit left child before visiting right child. We can treat the difference among different traverses is only when to do operation on root node. Then we can formulate questions into several scenarios: To do the operation on root node, you need to do the operation on left subtree and right subtree first. (Merge Sort) To do the operation on root node, you need to do operation on its parent node first. (Get the depth of Binary Tree) To do the operation on root node, you need to do the operation on left&#x2F;right subtree first. (Check BST) The above three scenarios roughly covered all three traverse methods. Given a problem, we need to pay attention that what is the operation for a given node, and when to do it. Pre-order, In-order or Post-order ? Binary Search Tree(BST) is a kind of special Binary Tree that for a given root node the left subtree only contains nodes with value less than root node, and right subtree only contains nodes with value greater than root node. Classic Topics &#x2F; ProblemsHere are some famous topics and problems that you may want to know. Lowest Common AncestorLowest Common Ancestor problem is trying to get the lowest common ancestor node of two given nodes. To analysis the solution in a way that fit into the 3 scenarios mentioned above. Let’s think about what operation do we need to do for a single node.Basically, we need to traverse every node in the binary tree to determine whether the node is the LCA that we want. For a given node, LCA could be this node, but to confirm this, we need to make sure that both target nodes are within it’s subtree. Which means we can only confirm this unless we finish checking it’s left subtree and right subtree. Which means it’s actually a post-order traverse. Serialized and Deseriazlied Binary TreeCreate an BST from sorted arrayCreating an BST from sorte array is another interesting topic. Possible BSTsGiven an sorted array, return all possible BSTs. BFS &#x2F; Level Order TraverseBreadth First Search &#x2F; Level order traverse tries to scan each level and print the value. Level order traverse can be treated as an extended variant of Pre-order traverse. As for per-order traverse, we do operation on the single root node and then move to its left child and right child. But for level traverse, we can treat the nodes at the same level as a node and after we finish doing operations on this “Big Node”, we move to its child.","date":"2023-08-06","categories":["Algorithm"],"tags":["Binary Tree"]},{"title":"Binary Exponentiation","url":"/2023/08/06/Binary-Exponentiation/","content":"Binary ExponentiationBinary Exponentiation(for short BE) is a technique that can accelerate the calculation of exponentiation. For a naive exponentiation of , the time complexity is , with BE, the time complexity is .Let’s start with an example, Assuming is a 32-bit integer, which means at most we only need to calculate 32 times and see whether is included. Matrix Binary Exponentiation Matrix fast power is mainly used for linear recursive counting problems, as well as some dynamic programming problems where the state transition equation is a linear recursive relationship. There are some counting problems where the pattern can be observed by manually deducing small cases of n, and guess the recursive relationship. If this recursive relationship is linear, then it can be transformed into a matrix power problem, which can be accelerated using matrix fast power.","date":"2023-08-06"},{"title":"Patience Sorting","url":"/2023/08/06/Patience-Sorting/","content":"Patience SortingFrom wikipedia: Patience sorting is a sorting algorithm in computer science that uses the rules of the card game Patience to sort a list of elements by their values. The goal of the game is to form as few piles as possible. Pateince sorting can be used to solve Longest Increasing Sequence(LIS) problem. We can just skip the proof, feel free to refer document for detailed proof. The only thing we nned to know is that the number of piles is equal to the length of longest increasing sequence.","date":"2023-08-06","tags":["Divide and Conquer","LIS"]},{"title":"Binary Search Templates","url":"/2023/08/06/Binary-Search-Templates/","content":"Binary SearchBinary Search can be used to search or determine an element or pivot in a sorted sequence. Here we have three scenario of using binary search. Find an element in a unique element array Sometimes when there’re duplicated elements in an array, we may want to find the left bound or right bound of the target elements, things get complicated now. But we still have the way to do it. Find left bound We take the above code as an example. First we need to define the scope. If we use while(left <= right) which means our searching scope is . If we use while(left < right) it means our searching scope is because left can never reach to the initial right value For this line, when we find the target value is equal to the arr[mid], we don’t return mid, instead we shrink the right scope, which means that given , we have arr[i] >= target. On the other hand, for left, it will finally just enter the domain arr[i] >= target, or it will go beyound the scope the array. After eliminating the out of scope case, then we have two situations: arr[i] = target arr[i] > target This is to check whether the target value exist in the array. Find right bound","date":"2023-08-06","categories":["Algorithm"],"tags":["Binary Search","Divide and Conquer"]}]