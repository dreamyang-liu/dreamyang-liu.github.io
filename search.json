[{"title":"Model Quantization","url":"/2025/04/19/Model-Quant/","content":"Model Quantization量化是一种用来减少模型大小和加速推理的技术。主要是通过把模型权重从较高精度的数据类型如FP32转换为较低精度的数据类型如FP16/INT8来实现的。当然也有更加激进的量化方式，比如把权重量化到INT4，甚至是二值化，BitNet恐怖如斯。 这里可能要注意一下，量化到INT8和FP8是不一样的，INT8是整数，FP8是浮点数，两者的表示范围和精度是不一样的。一般来说INT8的性能吞吐量会更好，整数运算比浮点数运算更快，功耗更低，但是表示范围更小。 把模型量化有好处也有坏处 好处 减少模型大小：量化后的模型占用的内存更小，可以更快地加载到内存中，减少了内存占用。 加速推理：量化后的模型可以在更快的硬件上运行，比如在GPU上，量化后的模型可以更快地进行矩阵乘法运算。 降低功耗：量化后的模型可以在更低功耗的硬件上运行，比如在移动设备上，量化后的模型可以更长时间地运行。 坏处 精度损失：量化后的模型可能会有一定的精度损失，特别是在量化到INT8时，精度损失可能会比较大。 复杂度增加：量化后的模型可能会增加一些额外的计算，比如量化和反量化的过程，这会增加模型的复杂度。 量化的方法对称量化计算scale反量化 非对称量化计算scale和zero_point，原始tensor range为[x_min, x_max],量化后的tensor range为[q_min, q_max] Model Quantization Practice 这里对PTQ的QAT做一些补充：后训练量化（PTQ） 定义：在模型训练完成后，利用一小部分校准数据（通常数百到千张样本）计算各层权重和激活的量化参数（scale、zero‑point），然后将浮点模型转换为低位宽整数模型。 优点： 快速，无需再训练或微调 工程实现简单，适合资源受限场景 缺点： 量化误差较大，尤其对敏感层或小模型精度损失明显 对非对称分布或长尾分布不够鲁棒 校准集的作用校准集在静态量化过程中主要有以下几个方面的作用： 估计激活分布： 通过校准集”跑一次”模型前向传播，让Observer收集各节点的数值统计信息。 这些统计信息用于后续计算最优的量化参数。 确定量化参数： 利用收集到的统计信息计算scale和zero-point。 可采用Min-Max或直方图等方法，平衡精度和动态范围。 代表性样本： 校准集应覆盖目标应用的典型输入，确保量化后模型在实际场景中表现良好。 通常100-1000张样本即可，权衡统计稳定性和校准时间。 静态vs动态量化： 静态量化需要校准集，但推理更快。 动态量化无需校准，但每次推理都要计算激活的量化参数。 动态量化的Example如下，其每次的scale和zero-point是不一样的，因此需要在推理时根据当前batch计算。 量化感知训练（QAT） 定义：在训练或微调阶段就引入量化仿真（FakeQuant）算子，让模型在前向传播中模拟低精度运算，并在反向传播继续更新参数，以适应量化带来的误差。 优点： 能显著恢复甚至超越 PTQ 后的精度 对复杂分布、自定义策略更友好 缺点： 需要额外的训练或微调开销（1–5 epoch） 实现较复杂，需要在训练框架里插入 FakeQuant 层 Current ResearchLLM 专用 PTQ 方法 GPTQ 基于近似二阶信息的一次性权重量化，可在单 GPU 上对 175B 参数模型做 3–4 位量化且精度几乎无损 AWQ 通过激活感知的通道缩放，仅需保护 1% 的显著权重即可大幅降低量化误差，并在多模态 LLM 上实现高效 4‑bit 压缩与加速 SmoothQuant 则离线迁移激活离群值至权重，实现 W8A8 PTQ，在 LLM 上可带来最高 1.56× 加速和 2× 内存减少 最新的 SmoothQuant+ 将 PTQ 推向 4‑bit 群组化权重量化，实现几乎无损的 LLM 部署 Model Export and Deploy 方法 导出格式 / 工具 支持硬件 优势 劣势 TorchScript .pt (TorchScript) CPU/GPU 原生 PyTorch 支持，无需额外依赖，C++/Python 端加载方便 依赖 libtorch，GPU 性能不及专用推理框架 ONNX + ONNX Runtime .onnx CPU/GPU (多平台) 跨框架、多语言部署，生态成熟 自定义算子支持有限，性能依赖 Runtime 插件 ONNX + TensorRT .trt NVIDIA GPU 极致 GPU 推理性能，自动混合精度与量化优化 仅限 NVIDIA 平台，导出与部署流程较复杂 TensorFlow → TFLite .tflite 移动端/嵌入式设备 轻量级、低延迟、小体积，适合移动与 IoT 算子支持受限，需校准数据且精度可能下降 OpenVINO .xml + .bin Intel CPU/VPU 多硬件统一部署，Intel 硬件深度优化 仅限 Intel 平台，上手门槛较高 Apache TVM .so / .dll CPU/GPU/专用加速器 高度可定制的编译流水线，多平台与多后端支持 编译与调优复杂，需要深入学习框架使用与调参","date":"2025-04-19","categories":["ML System"],"tags":["LLM","ML System","Quantization"]},{"title":"Mermaid Diagram Test","url":"/2025/04/19/Mermaid-Test/","content":"Mermaid Diagram TestThis post demonstrates the mermaid diagram rendering capability in the blog. Flowchart Example Sequence Diagram Example Class Diagram Example State Diagram Example Gantt Chart Example Entity Relationship Diagram Pie Chart Example Journey Diagram Example Git Graph Example This post demonstrates how mermaid diagrams can be used to create various types of visualizations in your blog posts.","date":"2025-04-19","categories":["Documentation"],"tags":["test","mermaid"]},{"title":"MLSys 学习笔记 1","url":"/2025/04/17/MLSys-杂记1/","content":"1. GPU ECC（Error‑Correcting Code） 原理：在显存（VRAM）中为每 N 位数据生成冗余校验位，常见为 SECDED（单错纠正、双错检测）。 实现： 专业级卡（Tesla/Quadro/A100、AMD Instinct）默认支持 ECC，消费级卡多关闭或不支持。 ECC 逻辑集成在内存控制器，对程序透明。 优缺点： 优点：避免显存位翻转导致的 Silent Data Corruption，可监控纠错/未纠错计数。 缺点：约 12.5% 显存开销、5–12% 带宽与延迟损失。 开启与监控（NVIDIA 举例）: 2. NVIDIA Persistence Mode（持久化模式） 作用：驱动在无客户端时依然保持 GPU 上下文初始化，减少短作业启动延迟；保持稳定 P‑state。 命令： 注意：功耗略增；可配合 nvidia-persistenced 守护进程跨重启生效。 3. Ring Reduce（All‑Reduce）算法概览 分段（Scatter）：将张量平分 N 段，每轮只在环上沿顺时针传递一段。 Reduce‑Scatter：经过 N–1 轮，每段在环上累加所有节点后恰好落在一个归属节点上。 All‑Gather：再用同样方式广播各段结果，让所有节点拿到完整聚合值。 通信成本： Reduce‑Scatter：每节点发送/接收 数据量 All‑Gather：同理 合计 All‑Reduce： 4. Reduce‑Scatter 详细示例（4 节点、向量长 4） 初始数据 分段规则：下标 0→Seg 0，1→Seg 1，…，3→Seg 3。 3 轮累加： Segment 轮 1 累加结果 轮 2 累加结果 轮 3 累加结果 最终归属节点 全局和 0 5 + 1 = 6 9 + 6 = 15 13 + 15 = 28 Node 3 28 1 10 + 6 = 16 14 + 16 = 30 2 + 30 = 32 Node 0 32 2 15 + 11 = 26 3 + 26 = 29 7 + 29 = 36 Node 1 36 3 4 + 16 = 20 8 + 20 = 28 12 + 28 = 40 Node 2 40 结果：各节点分别持有自己负责 Segment 的全局和，为后续 All‑Gather 做准备。","date":"2025-04-17","categories":["ML System"],"tags":["GPU","ECC","持久化模式","Ring Reduce","Reduce-Scatter"]},{"title":"MLSys","url":"/2025/04/17/MLSys/","content":"Flowchart Example","date":"2025-04-17","tags":["GPU","ECC","持久化模式","Ring Reduce","Reduce-Scatter"]},{"title":"括号问题","url":"/2023/09/17/Parentheses-Problems/","content":"括号是一类有趣的问题，主要与有效括号相关，例如： 检查字符串是否是有效括号 在子串中找到最长的有效括号（子串） 在子串中找到最长的有效括号（子序列） 为给定长度生成所有有效括号 … 在深入探讨每个主题之前，让我们定义什么是有效括号。()、()()()()和((((((()))))))(())()。","date":"2023-09-17","categories":["Algorithm"],"tags":["字符串"]},{"title":"限流算法","url":"/2023/09/09/RateLimiter/","content":"限流器是分布式系统中的一个关键组件。通常，我们可以用它来防止DDoS或恶意请求。在这篇文章中，我们将介绍几种可用于限流的算法。 令牌桶令牌桶是一个具有预定义容量的容器。令牌以预设的速率定期放入桶中。一旦桶满了，就不会再添加令牌。如图所示，令牌桶容量为4。填充器每秒向桶中放入2个令牌。一旦桶满了，多余的令牌就会溢出。 令牌桶可以处理突发请求，并且可以确保限制不会超过预设的容量。 漏桶漏桶类似于令牌桶，但它的行为像一个FIFO队列。漏桶有一个处理速率，表示它每秒可以处理多少请求。当请求到来时，它会首先检查队列是否已满，如果没有，将请求添加到队列中，否则丢弃请求。队列中的请求会以固定的时间间隔从队列中拉出并处理。 漏桶的一个问题是它以FIFO方式处理，如果队列被旧请求填满，新请求将被限流。","date":"2023-09-09"},{"title":"Golang Tutorial 1","url":"/2023/09/08/Golang-Tutorial-1/","content":"","date":"2023-09-08"},{"title":"Rust Tutorial 1","url":"/2023/09/08/Rust-Tutorial-1/","content":"","date":"2023-09-08"},{"title":"Backtracking Template","url":"/2023/09/02/Backtracking-Template/","content":"回溯算法回溯算法通常用于搜索解空间。这样的解空间往往是组合爆炸式增长的，比如排列问题就是一个典型示例。 给定一个数字 ，打印出区间 中所有数字的排列。 对于 ，期望输出： 在这种情况下，我们需要使用回溯来解决问题。回溯会在每次做出决策、选择一个候选项扩展时保存一个“检查点”。 下面给出回溯算法的模板： 下面对排列问题做一个部分“手动演练”：1. 从 candidates 中选出 1，并移除它2. 从剩余中选出 2，并移除它3. 再选出 3，并移除它4. 此时 candidates 为空，跳出，记录排列 [1,2,3]5. 把 3 加回 candidates，但此时已无其他候选6. 把 2 加回 candidates，移动到下一个候选 37. 从 candidates 中选出 3，并移除它8. 再选出 2，并移除它9. candidates 为空，跳出，记录排列 [1,3,2]10. 依此类推… 显然，这里的时间复杂度并不是线性的，实际上与叶子节点数量一致，为 。","date":"2023-09-02","categories":["Algorithm"],"tags":["Backtracking"]},{"title":"Docker 杂记","url":"/2023/08/15/Docker-Memo/","content":"DockerfileDockerfile是一个包含容器镜像定义的文本文件，可用于启动容器。这个定义包含命令、基础镜像、配置等。在这篇文章中，我们将介绍每个组件，并讨论在什么场景下使用它们。让我们开始吧。 Docker镜像在深入了解各个组件之前，我们可能想知道Docker镜像到底是什么。参考：镜像的结构 镜像是一个只读模板，包含创建Docker容器的指令。通常，一个镜像是基于另一个镜像，并进行一些额外的定制。–Docker官方 Docker镜像可以包含很多文件，为了深入了解细节，让我们先拉取一个镜像。 从上面的输出中，我们知道我们从dockerhub拉取了python:alpine3.18镜像，这个镜像有5层。 基础镜像 &#x2F; 父镜像在大多数Dockerfile中，你会看到的第一行是FROM &#123;基础镜像&#125;，比如FROM python:3.8。 这一行定义了我们要构建的镜像的基础镜像。基础镜像可能包含一个或多个层。 RUNRUN命令可用于执行一些命令，如安装包、下载文件等。需要提到的一点是，每个命令都会在现有镜像上添加一个新层。将多个命令合并到一个RUN命令中是减小镜像大小和减少层数的常用技巧。 CMDCMD描述默认的容器参数或命令。用户可以在使用时轻松覆盖默认命令。 ENTRYPOINTENTRYPOINT也用于定义始终运行的命令，这意味着它不能被覆盖。ENTRYPOINT可用于确保容器启动时始终运行特定命令，即使用户尝试覆盖它。 COPYCOPY可用于将本地文件复制到容器镜像中，仅此而已。 ADDADD命令也允许将文件&#x2F;文件夹从主机复制到镜像。它支持tarball提取和远程URL支持。通常，ADD比COPY更受欢迎。现在让我们看一个在Dockerfile中使用ADD的例子。 假设我们在一个文件夹/tmp/app中，它有以下文件， 同时，我们在Dockerfile中写入以下命令。 上述命令将文件/tmp/app/server.js（主机路径）复制到/tmp/server.js（容器镜像路径）。我们注意到第一个参数应该在上下文内，上下文可以解释为我们执行docker build命令的路径。 VOLUME卷可以被视为Docker容器的持久数据存储。以下命令意味着从此镜像启动的任何容器，主机上的/var/www/html目录将被挂载到容器内的/var/www/html目录。这意味着在容器内对/var/www/html目录所做的任何更改都将反映在主机上，反之亦然。 DiveDive是一个非常有用的工具，用于分析容器镜像。它使你能够看到每一层的命令和反映在文件系统上的变化。 参考 OCI镜像清单规范 Docker FileSystemDocker的filesystem有很多,可以配置，最常用且推荐的目前应该是overlay2，它的性能和稳定性都比较好。如果你对Docker DataRoot list一下，你会发现有很多文件夹，其中有一个是overlay2，这个文件夹就是overlay2的存储位置。 其中有很多各个文件夹，名字都是一串哈希值，这些文件夹就是各个镜像的FileSystem的存储位置，其中是每个都是一个layer，这个layer有可能是一个image 的layer，也可能是正在运行的container的layer，也可能是一个volume的layer。 这里folder name的hash并不是layer的sha256，而是一个短ID，Overlay2 在磁盘上为该层分配的 diff 目录名。 Overlay2 存储结构要点1. 镜像元数据 & LayerDB• &#x2F;var&#x2F;lib&#x2F;docker&#x2F;image&#x2F;overlay2&#x2F;imagedb&#x2F;content&#x2F;sha256&#x2F;这里的 • &#x2F;var&#x2F;lib&#x2F;docker&#x2F;image&#x2F;overlay2&#x2F;layerdb&#x2F;sha256&#x2F;&#x2F;cache-id这个文件里的内容（一个短 ID，比如 36849485abab637c0…）才是 Overlay2 在磁盘上为该层分配的 diff 目录名。 2. Overlay2 工作目录 • &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;&#x2F;diff存放该层的实际文件内容。 • &#x2F;var&#x2F;lib&#x2F;docker&#x2F;overlay2&#x2F;&#x2F;merged容器运行时挂载点，读取时会把 LowerDir + UpperDir overlay 在这里。","date":"2023-08-15","categories":["Cloud Computing"],"tags":["Docker"]},{"title":"Linux系统概述","url":"/2023/08/08/Linux-System-Overview/","content":"Linux可能是世界上使用最广泛的操作系统。了解Linux系统的基础知识对开发者在很多方面都有益处。 文件系统你可以在这里找到更详细的文件系统层次结构。 内核环境变量PATHPATH是最重要的环境变量之一。默认情况下，它的值为 当你输入一个命令时，它会搜索上述所有文件夹，并查找是否存在同名的可执行文件。用户需要有执行该文件的权限，否则会出现权限错误zsh: permission denied: db.json。 I&#x2F;O设备","date":"2023-08-08","categories":["OS"],"tags":["Linux"]},{"title":"随机算法","url":"/2023/08/08/Random-Algorithm/","content":"带权重的 random()random() 是一个重要的函数，它会以相同的概率返回一个随机数。带权重的 random() 要求我们生成具有不同权重的随机数。 一种简单的方法是我们可以创建一个包含 个元素的数组。我们随机设置 个元素为 ， 个元素为 ， 个元素为 ， 个元素为 。然后我们可以从 到 随机选择一个索引，并返回所选索引上的值。然而，上述方法有一个明显的缺点。它会消耗大量内存，设置值也会花费大量时间。 我们能否压缩这些信息，而不是创建一个数组？答案是可以的，让我们考虑大小为 的权重数组 的前缀和数组 （0索引）。 然后对于给定的数组，我们有 和 。我们可以从 中提取 个区间 。每个区间对应于权重长度。这意味着，如果我们随机选择一个索引 , 落入 的概率是. 洗牌算法Fisher–Yates 洗牌算法可用于生成数组的随机排列。该算法背后的原理是我们需要确保每对索引，即 具有相同的交换概率。 在上述实现中，对于每个 ，我们随机选择一个 进行交换。 注意，我们也包括 ，因为有可能我们根本不改变它。 通过这种方式，我们能够覆盖所有对 ，并且当我们从 到 迭代 i 时，我们将能够覆盖所有对。","date":"2023-08-08","categories":["Algorithm"],"tags":["随机化"]},{"title":"Monotonic Stack/Queue","url":"/2023/08/06/Monotonic-Stack-Queue/","content":"","date":"2023-08-06","categories":["Algorithm"],"tags":["Data Structure"]},{"title":"并查集","url":"/2023/08/06/Union-Find/","content":"","date":"2023-08-06","categories":["Algorithm"],"tags":["数据结构"]},{"title":"Trie Tree","url":"/2023/08/06/Trie-Tree/","content":"","date":"2023-08-06","categories":["Algorithm"],"tags":["数据结构"]},{"title":"树状数组","url":"/2023/08/06/Fenwick-Tree/","content":"树状数组，也称为二进制索引树，是一种支持单点更新和区间查询的数据结构。在大多数情况下，我们可以用它来查询区间和、区间乘积或其他满足以下条件的操作： 结合律： 具有逆运算，我们可以通过 和 推断出 它基于这样一个假设：任何前缀 都可以分解为最多 个区间，这些区间的信息是已知的。 从上图可以看出，每个位置 控制的区间的右边界是 。在树状数组中， 被分配控制长度为 的区间。这里， 是从右边数第一个 1 位的位置减 1。例如，$12 = (1100)2，则k = 2，2^k = 4。所以在这种情况下，c{12}控制的区间是[12 - 4 + 1, 12]。2^k可以使用函数获得，即x\\ & \\ (-x)$。","date":"2023-08-06","categories":["Algorithm"],"tags":["数组","数据结构","位运算技巧"]},{"title":"线段树","url":"/2023/08/06/Segment-Tree/","content":"线段树是一种常用于维护区间信息的数据结构。 线段树可以在 的时间复杂度内执行诸如单点更新、区间更新和区间查询（区间和、区间最大值、区间最小值）等操作。","date":"2023-08-06","categories":["Algorithm"],"tags":["数据结构","线段树"]},{"title":"一致性哈希","url":"/2023/08/06/Consistent-Hashing/","content":"一致性哈希是一种常用于分布式系统的技术，用于将数据分布到不同的服务器/服务/数据中心。它可以确保： 数据 -> 节点映射是使用哈希函数计算的，数据 将被发送到第一个哈希值 的节点 当添加/删除节点时，只有部分数据需要重新映射，大多数数据不会受到影响。 与传统的哈希映射相比，一致性哈希可以实现更均衡的数据分布 方法论以下是一致性哈希工作原理的简要图示。 一致性哈希尝试执行以下操作： 创建一个哈希环 并均匀地将现有节点分布在环上。 计算数据的哈希值，并将其发送到具有更大哈希值的第一个节点。 当S3节点被移除时，只有 范围内的数据会被重新映射到S2。 上述一致性哈希看起来不错，但是，一些服务器可能会过载，而一些服务器则一直处于空闲状态。在这种情况下，我们可能会观察到，大部分数据的哈希值落入一个范围，即 ，这是整个哈希环的一个子域。 在上图中，大多数ID落入IP2的范围内，这表示分布不均衡。为了解决这个问题，我们引入了虚拟节点来更均匀地分配工作负载。 与在哈希环上只有一个节点代表物理服务器不同，我们实际上创建了多个虚拟节点并将它们分布在哈希环域上。在这种情况下，如果数据落入一个子域，我们仍然可以将它们映射到不同的物理服务器。同时，如果我们添加或删除节点，受影响的范围将比简单的一致性哈希小得多，因为每个段将小得多。","date":"2023-08-06","categories":["System Design"]},{"title":"系统设计基础","url":"/2023/08/06/System-Design-Fundamentals/","content":"CAP定理CAP定理，或称为Brewer定理，指出分布式数据库系统只能保证三个特性中的两个：一致性、可用性和分区容忍性。系统优先考虑可用性而非一致性，可能会响应可能过时的数据。 Partition Tolerance在CAP理论中,Partition Tolerance(分区容忍性)是指系统在发生网络分区故障的时候,仍然能够对外提供服务的能力。 所谓网络分区故障,是指系统中的节点被分成两部分,节点间的网络通信被阻断。在这种情况下,一个分布式系统可能会表现出下面两种行为: 无法对外提供服务 整个系统不可用,对用户不可见。 以牺牲一致性为代价继续服务 系统继续对用户可见,但返回的数据可能不一致。 Partition Tolerance表示系统能够承受网络分区的发生,继续对外服务而不会完全崩溃。根据CAP理论,一个系统不可能同时满足一致性(Consistency)、可用性(Availability)和分区容忍性,最多只能同时满足两个。 所以一个Partition Tolerance系统通常会牺牲数据一致性来保证服务可用性。比如允许读取到过期数据,或两个分区的数据产生冲突。这在许多大规模分布式系统中都是可接受的权衡。 一个典型的Partition Tolerance(分区容忍性)的例子是DNS系统。 DNS采用了分布式的树形结构,一个域名由多个DNS服务器共同提供解析服务。当网络发生分区时,可能出现如下情况: 某区域用户无法访问部分DNS服务器,但可以访问到另一部分服务器,仍能得到域名解析结果。 不同的DNS服务器返回了不同的解析结果(IP地址不一致),但用户还是能得到响应。 这就是DNS系统表现出来的Partition Tolerance性质。当网络分区发生时,DNS系统为了继续服务,承受返回不一致数据的结果,而没有选择完全停止响应。 Partition Tolerance的另一个例子是一些分布式缓存系统,如Memcached。当节点间失去联系时,不同分区的数据可能不一致,但每个分区内部仍能继续使用本地缓存,整个系统不会完全停止服务。 这些系统都采用了设计理念:”允许读取脏数据”或者”允许短暂不一致”来实现Partition Tolerance。因为对大多数应用来说,与整个系统完全不可用相比,读取到陈旧或不一致的数据仍是可以接受的。 现代分布式系统组件负载均衡器负载均衡器是Web应用程序的入口点。它将传入的客户流量分配到不同的工作节点&#x2F;服务器。它可以帮助： 避免单个服务器过载 当一个服务器不可用时，将请求重定向到健康的服务器。 负载均衡器在使系统能够水平扩展方面发挥着重要作用。AWS提供以下负载均衡器服务： 经典负载均衡器：主要用于将流量分配到EC2实例 应用负载均衡器：更灵活，默认选择这个。 网络负载均衡器 网关负载均衡器 CDN内容分发网络（CDN）是第三方提供的快速文件交付服务。通过提供单一端点，CDN服务将自动路由到最近的服务器获取静态文件。它主要用于提供静态文件，如HTML、JavaScript文件和图像。它还可以托管一些文件，如视频、压缩包等。它可以： 减轻Web服务器的负载 通过从最近的服务器获取文件来加速文件检索。 CDN需要设置TTL（生存时间）。如果TTL设置为较小的值，它将需要经常从源更新缓存，数据传输的费用将增加。如果TTL设置为较大的值，客户可能会获取过时的文件。TTL应根据系统需求设置。 DNS域名系统（DNS）是一个将域名转换为互联网协议（IP）地址的数据库。DNS将人们用来定位网站的名称映射到计算机用来定位该网站的IP地址。 Web层Web层主要指Web服务。Web服务主要包含业务逻辑。如从数据库获取数据并返回给用户，或代表用户提交订单等。理想情况下，Web层应该是无状态的，因为这可以使Web层更容易水平扩展。 数据层数据层主要指数据库或持久存储。”持久”意味着即使服务器关闭，数据也不会丢失。相反，一些内存缓存系统如果服务器关闭，将丢失所有数据。随着数据变得越来越复杂，人们开发了各种数据库系统来适应不同的用例。 关系型数据库 NoSQL（不仅仅是SQL） 在这里详细描述数据库太过雄心勃勃，我们将用单独的文章详细讨论它。 缓存层缓存层的出现是因为查询数据库是一个昂贵的操作。同时，一些请求也可能很昂贵，比如运行模型进行计算。多亏了局部性原理，我们知道处理器在短时间内重复访问同一组内存位置的趋势。这意味着同一数据可能在短时间内被多次使用。这使得缓存层在实际生产中很有用。缓存作为系统内的中间件工作，可以独立扩展。它也带来了一系列挑战，如缓存过热、缓存未命中和数据同步。与数据层一样，我们将用单独的文章详细讨论它。 消息队列有时我们可能会考虑是否存在服务调用链。这些子系统如何独立扩展。例如，如果服务A试图调用服务B，如果服务B在服务A发出调用时不可用，那么服务A必须等待，否则我们将丢失来自客户的请求。我们可能希望有一些东西可以抽象服务的输入生产者和结果消费者。这就是消息队列发挥作用的地方。 消息队列作为临时存储服务。生产者可以向MQ发布消息，无论消费者是否工作。同时，消费者可以从MQ消费消息，无论生产者是否工作。","date":"2023-08-06","categories":["系统设计"],"tags":["概述"]},{"title":"二叉树主题","url":"/2023/08/06/Binary-Tree-Topics/","content":"遍历关于二叉树遍历有很多有趣的主题，比如BST的中序遍历是一个有序序列。我们可能还想知道如何进行层次遍历（使用队列），这是进行广度优先搜索（BFS）的基础。在下面的每个部分中，我们将讨论这些主题。 前序遍历前序遍历将按照根节点 -> 左子节点 -> 右子节点的顺序访问。 中序遍历中序遍历将按照左子节点 -> 根节点 -> 右子节点的顺序访问。BST的中序遍历将返回一个有序数组。 后序遍历后序遍历将按照左子节点 -> 右子节点 -> 根节点的顺序访问。 要确定一棵二叉树，我们需要中序序列 + 前序&#x2F;后序序列。 解决二叉树问题的方法论一个重要的观察是，当我们查看根节点的位置时，从前序到后序，它从第一个变为最后一个。由于我们总是在访问右子节点之前访问左子节点，我们可以将不同遍历之间的差异仅视为何时对根节点进行操作。 然后我们可以将问题归纳为几种情况： 要对根节点进行操作，你需要先对左子树和右子树进行操作。（归并排序） 要对根节点进行操作，你需要先对其父节点进行操作。（获取二叉树的深度） 要对根节点进行操作，你需要先对左&#x2F;右子树进行操作。（检查BST） 上述三种情况大致涵盖了所有三种遍历方法。给定一个问题，我们需要注意对给定节点的操作是什么，以及何时进行操作。前序、中序还是后序？ 二叉搜索树（BST）是一种特殊的二叉树，对于给定的根节点，左子树只包含值小于根节点的节点，右子树只包含值大于根节点的节点。 经典主题&#x2F;问题以下是一些你可能想了解的著名主题和问题。 最低公共祖先最低公共祖先问题是尝试获取两个给定节点的最低公共祖先节点。 为了以适合上述3种情况的方式分析解决方案，让我们思考一下我们需要对单个节点做什么操作。基本上，我们需要遍历二叉树中的每个节点，以确定该节点是否是我们想要的LCA。对于给定节点，LCA可能是这个节点，但要确认这一点，我们需要确保两个目标节点都在它的子树中。这意味着除非我们完成检查它的左子树和右子树，否则我们无法确认这一点。这意味着它实际上是一个后序遍历。 二叉树的序列化和反序列化从有序数组创建BST从有序数组创建BST是另一个有趣的主题。 可能的BST给定一个有序数组，返回所有可能的BST。 BFS &#x2F; 层序遍历广度优先搜索&#x2F;层序遍历尝试扫描每一层并打印值。 层序遍历可以被视为前序遍历的扩展变体。对于前序遍历，我们对单个根节点进行操作，然后移动到其左子节点和右子节点。但对于层次遍历，我们可以将同一层的节点视为一个节点，在我们完成对这个”大节点”的操作后，我们移动到它的子节点。","date":"2023-08-06","categories":["Algorithm"],"tags":["二叉树"]},{"title":"二进制快速幂","url":"/2023/08/06/Binary-Exponentiation/","content":"二进制快速幂二进制快速幂（简称BE）是一种可以加速幂运算计算的技术。对于朴素的幂运算 ，时间复杂度是 ，而使用二进制快速幂，时间复杂度降为 。让我们从一个例子开始， 假设 是一个32位整数，这意味着最多我们只需要计算32次，并检查 是否包含在内。 矩阵快速幂 矩阵快速幂主要用于线性递归计数问题，以及一些状态转移方程是线性递归关系的动态规划问题。有些计数问题可以通过手动推导小规模的n来观察模式，并猜测递归关系。如果这个递归关系是线性的，那么它可以转化为矩阵幂问题，并使用矩阵快速幂加速计算。","date":"2023-08-06","categories":["Algorithm"],"tags":["快速幂"]},{"title":"KV Cache","url":"/2023/08/06/KV-Cache/","content":"KV Cache在LLM推理时，在attention层中计算attention score时，需要计算query和key的点积，然后进行softmax归一化，最后与value进行加权求和。由于自回归的特性，每次生成一个token都需要和之前的所有token进行attention计算，这个时候就需要将之前的key和value进行缓存，以便下一次计算时直接使用。是一种用来加速推理的技术。 但是KV Cache占用显存空间很大，尤其对于Long Context的inference，一般可以到 30% 以上。因此需要对KV Cache进行压缩，以减少显存占用。 KV Cache压缩和计算优化压缩和计算优化是两个不同的概念，压缩是为了减少显存占用，计算优化是为了减少计算量。压缩和计算优化可以同时进行，也可以单独进行。 KV Cache压缩的方法有很多，主要分为两类，一类是量化(有机会再单独写一篇文章，现在我也整不明白)，一类是剪枝。量化是将浮点数转换为低精度的整数，剪枝是将不重要的token/channel/layer进行eviction 或者分配较少的budget。 首先我们看一下KV Cache的结构。 其中， B是batch size N是sequence length H是head number L是attention layers number d是 embedding dimension 一般来说Batch size无法进行优化，因此我们主要关注，，，这四个维度。 因此，我们可以从这四个维度进行压缩。 N维度压缩，即对sequence length进行压缩，可以使用剪枝的方法，将不重要的token进行eviction。一般有以下几种 随机剪枝，随机选择一些token进行eviction。 Sliding window，只保留最近的N个token。 H维度压缩，即对head number进行压缩，可以使用剪枝的方法，将不重要的head进行eviction。某些head对于推理的结果影响不大，我们可以将这些head进行eviction或分配较少的Cache Budget。 RazorAttention 通过仅为少数检索头保留全量缓存、对其他头丢弃远程 tokens 并用补偿 token 恢复信息，实现了 >70% 的 KV‑Cache 缩减、无性能损失且无需重训、兼容 FlashAttention。 L维度压缩，即对attention layers number进行压缩，可以使用剪枝的方法，将不重要的layer进行eviction。比如说某些layer对于推理的结果影响不大，我们可以将这些layer进行eviction或分配较少的Cache Budget。 SqueezeAttention CAKE CAKE 将 KV 缓存驱逐抽象为“蛋糕切分”问题，通过结合各层的时空注意力动态按层级级联分配缓存资源并引入新的时序驱逐指标，在仅用 3.2% 缓存的前提下保持性能并显著加速长序列推理。 d维度压缩，即对embedding dimension进行压缩，一般是对某个一些不重要的维度进行删除，比如说 256 维度的embedding，我们可以删除一些不重要的维度，比如说删除128维度，这样就可以将embedding dimension从256降到128。 Low Rank Projection: 将KV Cache进行低秩分解，然后只保留低秩分解的结果，这样就可以减少显存的使用。 Palu Palu 通过对生成 Key/Value 的投影层做低秩分解，仅缓存低秩中间结果并在注意力计算时动态重构全维 KV，从而大幅降低内存占用并提升推理效率。 还有可以通过CPU offloading来减少显存的使用，将一些不重要的token的KV Cache放到CPU上，这样就可以减少显存的使用。 KV Cache的计算优化TopK Selection：在计算attention score的时候，我们可以只选择topK个score，这样就可以减少计算量。 这里计算量的减少主要是在加权求和值的计算上，因为 的维度变小了，从而减少了计算量。之前的计算量是 ，现在的计算量是 ，其中 是序列长度， 是embedding dimension， 是topK的个数。(这里计算的是一个token的计算量) 对于Selection还有一些其他的做法，比如说通过Sampling的方式来选择，可以保留部分整体的信息比如MagicPIG 通过 LSH 采样加上 CPU 异构执行，以带理论保证的方式近似注意力计算，为超长上下文 LLM 推理提供高效、低延迟的解决方案。","date":"2023-08-06","categories":["ML System"],"tags":["LLM","KV Cache","ML System"]},{"title":"耐心排序","url":"/2023/08/06/Patience-Sorting/","content":"耐心排序来自维基百科：耐心排序是计算机科学中的一种排序算法，它使用纸牌游戏”耐心”的规则按值对元素列表进行排序。游戏的目标是形成尽可能少的牌堆。耐心排序可用于解决最长递增序列（LIS）问题。 我们可以跳过证明，详细证明请参考文档。我们只需要知道牌堆数等于最长递增序列的长度。","date":"2023-08-06","categories":["Algorithm"],"tags":["分治法","最长递增子序列"]},{"title":"二分查找模板","url":"/2023/08/06/Binary-Search-Templates/","content":"二分查找二分查找可用于在有序序列中搜索或确定元素或枢轴。这里我们有三种使用二分查找的场景。 在唯一元素数组中查找元素 有时当数组中有重复元素时，我们可能想要找到目标元素的左边界或右边界，这时情况会变得复杂。但我们仍然有方法来做到这一点。 查找左边界 我们以上面的代码为例。首先我们需要定义搜索范围。 如果我们使用 while(left <= right)，这意味着我们的搜索范围是 。 如果我们使用 while(left < right)，这意味着我们的搜索范围是 ，因为left永远不会达到初始的right值。 对于这一行，当我们发现目标值等于 arr[mid] 时，我们不返回 mid，而是缩小右边界，这意味着对于 ，我们有 arr[i] >= target。另一方面，对于 left，它最终会进入 arr[i] >= target 的域，或者它会超出数组的范围。排除超出范围的情况后，我们有两种情况： arr[i] = target arr[i] > target 这是为了检查目标值是否存在于数组中。 查找右边界","date":"2023-08-06","categories":["Algorithm"],"tags":["二分查找","分治法"]},{"title":"about","url":"/about/index.html","content":"","date":"2025-04-18"},{"title":"tags","url":"/tags/index.html","content":"","date":"2025-04-18"},{"title":"search","url":"/search/index.html","content":"","date":"2025-04-18"},{"title":"categories","url":"/categories/index.html","content":"","date":"2025-04-18"}]