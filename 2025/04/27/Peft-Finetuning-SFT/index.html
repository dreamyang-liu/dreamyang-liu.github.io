<!DOCTYPE html>
<html lang="en">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <title>Peft Finetune LoRA Example | Tuntun&#39;s Blog</title>
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/main.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" integrity="sha512-9usAa10IRO0HhonpyAIVpjrylPvoDwiPUiKdWk5t3PyolY1cOd4DSE0Ga+ri4AuTroPR5aQvXU9xC6qOPnzFeg==" crossorigin="anonymous" referrerpolicy="no-referrer" />
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">

  <!-- 添加代码高亮样式 -->
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/atom-one-dark.min.css">
  <!-- 添加highlight.js库 -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
  <!-- 添加mermaid.js库 -->
  <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
  <link rel="stylesheet" href="/css/code.css">
  <link rel="stylesheet" href="/css/code-custom.css">
  <link rel="stylesheet" href="/css/code-languages.css">
  <link rel="stylesheet" href="/css/mermaid.css">

  <link rel="stylesheet" href="/css/vscode.css">
  <link rel="stylesheet" href="/css/post.css">
  <link rel="stylesheet" href="/css/tag.css">
  <link rel="stylesheet" href="/css/categories.css">
  <link rel="stylesheet" href="/css/archive.css">
  <link rel="stylesheet" href="/css/search.css">
  <link rel="stylesheet" href="/css/mobile.css">  <link rel="stylesheet" href="/css/responsive.css">
  <link rel="stylesheet" href="/css/elements.css">

  <!-- 添加 JetBrains Mono 字体 -->  
  <link href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@400;700&display=swap" rel="stylesheet">

  <!-- Add any custom head content here -->

  <script src="/js/explorer.js"></script>
  <script src="/js/code-copy.js"></script>
  <script src="/js/code-enhance.js"></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 7.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

  <body>
    <div class="wrapper">
      <div class="mobile-menu-toggle">
        <i class="fas fa-bars"></i>
      </div>
      <header class="vs-header">
  <nav class="vs-nav">
    <div class="nav-left">
      <a href="/" class="nav-brand">
        <i class="fas fa-terminal"></i>
        Tuntun&#39;s Blog
      </a>
    </div>
    
    <div class="nav-right">
      <a href="/" class="nav-item ">
        <i class="fas fa-home"></i>
        <span>Home</span>
      </a>
      <a href="/archives/" class="nav-item ">
        <i class="fas fa-archive"></i>
        <span>Archives</span>
      </a>
      <a href="/categories/" class="nav-item ">
        <i class="fas fa-folder"></i>
        <span>Categories</span>
      </a>
      <a href="/tags/" class="nav-item ">
        <i class="fas fa-tags"></i>
        <span>Tags</span>
      </a>
      <a href="/search/" class="nav-item ">
        <i class="fas fa-search"></i>
        <span>Search</span>
      </a>
      <a href="/about/" class="nav-item ">
        <i class="fas fa-info-circle"></i>
        <span>About</span>
      </a>
    </div>
  </nav>
</header>

<script>
  function smoothScroll(event, target) {
    event.preventDefault();
    const targetId = target.substring(target.indexOf('#') + 1);
    const targetElement = document.getElementById(targetId);

    if (targetElement) {
      window.scrollTo({
        top: targetElement.offsetTop - 50, // 调整偏移量
        behavior: 'smooth'
      });
    } else {
      window.location.href = target;
    }
  }

  window.addEventListener('scroll', function() {
    const header = document.querySelector('.vs-header');
    const nav = document.querySelector('.vs-nav');
    const scrollPercent = (window.scrollY / (document.documentElement.scrollHeight - window.innerHeight)) * 100;
    
    nav.style.setProperty('--scroll-percent', `${scrollPercent}%`);
    
    if (window.scrollY > 0) {
      header.classList.add('scrolled');
    } else {
      header.classList.remove('scrolled');
    }
  });

  // 添加标签页切换动画
  document.querySelectorAll('.nav-item').forEach(item => {
    item.addEventListener('click', function(e) {
      const ripple = document.createElement('span');
      ripple.classList.add('nav-ripple');
      this.appendChild(ripple);
      
      const rect = this.getBoundingClientRect();
      const x = e.clientX - rect.left;
      const y = e.clientY - rect.top;
      
      ripple.style.left = `${x}px`;
      ripple.style.top = `${y}px`;
      
      setTimeout(() => ripple.remove(), 1000);
    });
  });
</script>


<div class="vscode-container">
  <!-- 左侧资源管理器 -->
  <div class="sidebar-explorer">
    <!-- TOC导航 -->
    <div class="explorer-section">
      <div class="section-header">
        <i class="fas fa-list"></i>
        <span>TABLE OF CONTENTS</span>
      </div>
      <div class="section-content">
        <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Steps"><span class="toc-text">Steps</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Get-a-pretrained-model-from-Huggingface"><span class="toc-text">Get a pretrained model from Huggingface</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Get-a-dataset-from-the-Hugging-Face"><span class="toc-text">Get a dataset from the Hugging Face</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Prepare-the-dataset"><span class="toc-text">Prepare the dataset</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Exciting-Training"><span class="toc-text">Exciting Training</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Load-Saved-Model"><span class="toc-text">Load Saved Model</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Appendix"><span class="toc-text">Appendix</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E4%B8%AA%E6%B5%8B%E8%AF%95GPU-Flops%E7%9A%84example"><span class="toc-text">两个测试GPU Flops的example</span></a></li></ol>
      </div>
    </div>
    
    <!-- 同分类文章 -->
    
    
    <!-- 标签列表 -->
    
    <div class="explorer-section">
      <div class="section-header">
        <i class="fas fa-tags"></i>
        <span>ARTICLE TAGS</span>
      </div>
      <div class="section-content">
        
          <div class="tag-item">
            <i class="fas fa-tag"></i>
            <a href="/tags/LLM/">LLM</a>
            <span class="count">(4)</span>
          </div>
        
          <div class="tag-item">
            <i class="fas fa-tag"></i>
            <a href="/tags/PEFT/">PEFT</a>
            <span class="count">(2)</span>
          </div>
        
          <div class="tag-item">
            <i class="fas fa-tag"></i>
            <a href="/tags/Finetune/">Finetune</a>
            <span class="count">(1)</span>
          </div>
        
          <div class="tag-item">
            <i class="fas fa-tag"></i>
            <a href="/tags/LoRA/">LoRA</a>
            <span class="count">(2)</span>
          </div>
        
      </div>
    </div>
    
  </div>

  <!-- 主要内容区域 -->
  <div class="editor-content">
    <div class="tab-bar">
      <div class="tab active">
        <i class="fas fa-file-alt"></i>
        <span>Peft Finetune LoRA Example.md</span>
      </div>
    </div>
    
    <div class="content-area">
      <article class="post-content">
        <div class="post-header">
          <h1>Peft Finetune LoRA Example</h1>
          <div class="post-meta">
            <span class="date">
              <i class="fas fa-calendar-alt"></i>
              2025-04-27
            </span>            
            
              <span class="tags">
                <i class="fas fa-tags"></i>
                <div class="tags-list">
                  <ul class="tag-item-post-list" itemprop="keywords"><li class="tag-item-post-list-item"><a class="tag-item-post-list-link" href="/tags/Finetune/" rel="tag">Finetune</a></li><li class="tag-item-post-list-item"><a class="tag-item-post-list-link" href="/tags/LLM/" rel="tag">LLM</a></li><li class="tag-item-post-list-item"><a class="tag-item-post-list-link" href="/tags/LoRA/" rel="tag">LoRA</a></li><li class="tag-item-post-list-item"><a class="tag-item-post-list-link" href="/tags/PEFT/" rel="tag">PEFT</a></li></ul>
                </div>
              </span>
            
          </div>
        </div>
        
        <div class="post-body vscode-markdown">
          <h2 id="Steps"><a href="#Steps" class="headerlink" title="Steps"></a>Steps</h2><ol>
<li>Get a pretrained model from Huggingface</li>
<li>Load the model and tokenizer</li>
<li>Prepare the dataset</li>
<li>Prepare the training arguments</li>
<li>Prepare the trainer</li>
<li>Add the model </li>
<li>Train the model</li>
<li>Save the model</li>
</ol>
<h2 id="Get-a-pretrained-model-from-Huggingface"><a href="#Get-a-pretrained-model-from-Huggingface" class="headerlink" title="Get a pretrained model from Huggingface"></a>Get a pretrained model from Huggingface</h2><p>先加载模型和tokenizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer<br><br>model_name = <span class="hljs-string">"Qwen/Qwen3-4B-Base"</span><br><br>model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    torch_dtype=<span class="hljs-string">"auto"</span>, <span class="hljs-comment"># 自动选择torch.float16或torch.bfloat16，不设置的话就会变成torch.float32</span><br>    device_map=<span class="hljs-string">"auto"</span><br>)<br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br></code></pre></td></tr></table></figure>

<p>对模型进行量化以减少显存占用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BitsAndBytesConfig<br>quantization_config = BitsAndBytesConfig(<br>    load_in_8bit=<span class="hljs-literal">True</span>,<br>    bnb_8bit_compute_dtype=torch.float16,<br>)<br><br>quantization_config = BitsAndBytesConfig(<br>    load_in_4bit=<span class="hljs-literal">True</span>,<br>    bnb_4bit_compute_dtype=torch.float16,<br>)<br></code></pre></td></tr></table></figure>

<blockquote>
<p>很神奇的是8bit在L4上计算利用率不高49%，4bit利用率较高80%。bfloat16和float16的计算利用率更高。<br>另外，8bit和4bit的显存占用差不多，8bit 大约9G，4bit大约7.5G。</p>
</blockquote>
<p>Anyway，我们现在用4bit量化来作为base model，可以减少一些显存占用。准备好了量化模型后我们可以开始配置LoRA了</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType<br><br>model = prepare_model_for_kbit_training(model)<br>lora_config = LoraConfig(<br>    r=<span class="hljs-number">16</span>,<br>    lora_alpha=<span class="hljs-number">16</span>,<br>    target_modules=[<br>        <span class="hljs-string">"q_proj"</span>,<br>        <span class="hljs-string">"k_proj"</span>,<br>        <span class="hljs-string">"v_proj"</span>,<br>        <span class="hljs-string">"o_proj"</span>,<br>        <span class="hljs-string">"gate_proj"</span>,<br>        <span class="hljs-string">"up_proj"</span>,<br>        <span class="hljs-string">"down_proj"</span><br>    ],<br>    lora_dropout=<span class="hljs-number">0.05</span>,                <span class="hljs-comment"># Dropout</span><br>    bias=<span class="hljs-string">"none"</span>,                      <span class="hljs-comment"># 不为 bias 插入 LoRA</span><br>    task_type=TaskType.CAUSAL_LM      <span class="hljs-comment"># 任务类型：因果语言模型</span><br>)<br>lora_model = get_peft_model(model, lora_config)<br>trainable = <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> lora_model.parameters() <span class="hljs-keyword">if</span> p.requires_grad)<br>total     = <span class="hljs-built_in">sum</span>(p.numel() <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> lora_model.parameters())<br><span class="hljs-built_in">print</span>(<span class="hljs-string">f"LoRA 模型可训练参数: <span class="hljs-subst">{trainable:,}</span> / <span class="hljs-subst">{total:,}</span> (<span class="hljs-subst">{<span class="hljs-number">100</span>*trainable/total:<span class="hljs-number">.2</span>f}</span>%)"</span>)<br></code></pre></td></tr></table></figure>

<p>这里的 <code>prepare_model_for_kbit_training</code> 将LayerNorm转换为float32以防止激活崩溃。同时会把output embedding层的梯度计算开启。输出嵌入层（lm_head 或者 linear 层映射到词表空间）往往关系到最终生成质量，打开它的梯度计算可以让模型学习新的词表分布或纠正偏差。</p>
<h2 id="Get-a-dataset-from-the-Hugging-Face"><a href="#Get-a-dataset-from-the-Hugging-Face" class="headerlink" title="Get a dataset from the Hugging Face"></a>Get a dataset from the Hugging Face</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">"timdettmers/openassistant-guanaco"</span>, split=<span class="hljs-string">"train"</span>)<br></code></pre></td></tr></table></figure>

<h2 id="Prepare-the-dataset"><a href="#Prepare-the-dataset" class="headerlink" title="Prepare the dataset"></a>Prepare the dataset</h2><p>大部分的LLM都有自己的tokenizer和对应的chat template。下面是qwen3 apply chat template后的text</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">prompt = <span class="hljs-string">"Write a quick sort unsing assembly language"</span><br>messages = [<br>    {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt},<br>    {<span class="hljs-string">"role"</span>: <span class="hljs-string">"assistant"</span>, <span class="hljs-string">"content"</span>: <span class="hljs-string">"I have no idea about how to do it, could you ask another questions ?"</span>},<br>    {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: <span class="hljs-string">"Write a bubble sort in rust"</span>},<br>]<br>text = tokenizer.apply_chat_template(<br>    messages,<br>    tokenize=<span class="hljs-literal">False</span>,<br>    add_generation_prompt=<span class="hljs-literal">True</span>,<br>    enable_thinking=<span class="hljs-literal">False</span><br>)<br></code></pre></td></tr></table></figure>

<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs text">&lt;|im_start|&gt;user<br>Write a quick sort unsing assembly language&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>I have no idea about how to do it, could you ask another questions ?&lt;|im_end|&gt;<br>&lt;|im_start|&gt;user<br>Write a bubble sort in rust&lt;|im_end|&gt;<br>&lt;|im_start|&gt;assistant<br>&lt;think&gt;<br>&lt;/think&gt;<br></code></pre></td></tr></table></figure>

<p>现在看一下怎么处理data，主要是把prompt和response放到chat template里面。然后用tokenizer进行tokenize，然后对于prompt部分进行mask掉不计算loss。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">preprocess_fn</span>(<span class="hljs-params">examples</span>):<br>    <span class="hljs-string">"""把 prompt/response 拼成对话模板，tokenize，并 mask 掉 prompt 部分"""</span><br>    input_ids, attention_mask, labels = [], [], []<br><br>    <span class="hljs-keyword">for</span> prompt, response <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(examples[<span class="hljs-string">"prompt"</span>], examples[<span class="hljs-string">"response"</span>]):<br>        <span class="hljs-comment"># 构造 messages 列表</span><br>        messages = [<br>            {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>,      <span class="hljs-string">"content"</span>: prompt},<br>            {<span class="hljs-string">"role"</span>: <span class="hljs-string">"assistant"</span>, <span class="hljs-string">"content"</span>: response},<br>        ]<br>        <span class="hljs-comment"># 拼接成完整 text</span><br>        text = tokenizer.apply_chat_template(<br>            messages,<br>            tokenize=<span class="hljs-literal">False</span>,<br>            add_generation_prompt=<span class="hljs-literal">True</span>,<br>            enable_thinking=<span class="hljs-literal">False</span><br>        )<br>        <span class="hljs-comment"># tokenize，这里用最大长度2048，这里其实有些浪费，因为很多回答比2048要短很多</span><br>        tok = tokenizer(<br>            text,<br>            truncation=<span class="hljs-literal">True</span>,<br>            padding=<span class="hljs-string">"max_length"</span>,<br>            max_length=<span class="hljs-number">2048</span>,<br>        )<br>        <br>        ids = tok[<span class="hljs-string">"input_ids"</span>]<br>        mask = tok[<span class="hljs-string">"attention_mask"</span>]<br><br>        <span class="hljs-comment"># 找到 &lt;/think&gt; token 的位置，用它来分割 prompt vs. response</span><br>        sep_id = tokenizer.convert_tokens_to_ids(<span class="hljs-string">"&lt;/think&gt;"</span>)<br>        <span class="hljs-keyword">try</span>:<br>            sep_index = ids.index(sep_id) + <span class="hljs-number">1</span><br>        <span class="hljs-keyword">except</span> ValueError:<br>            sep_index = <span class="hljs-number">0</span><br>        <br>        <span class="hljs-comment"># 看一下分割的位置对不对</span><br><span class="hljs-comment">#         print(sep_index)</span><br><span class="hljs-comment">#         print(tokenizer.decode(ids[sep_index:], skip_special_tokens=True))</span><br><br>        <span class="hljs-comment"># 构造 labels：prompt 前半段设为 -100，response 部分保留真实 id</span><br>        lbl = ids.copy()<br>        lbl[:sep_index] = [-<span class="hljs-number">100</span>] * sep_index<br><br>        input_ids.append(ids)<br>        attention_mask.append(mask)<br>        labels.append(lbl)<br><br>    <span class="hljs-keyword">return</span> {<br>        <span class="hljs-string">"input_ids"</span>:      input_ids,<br>        <span class="hljs-string">"attention_mask"</span>: attention_mask,<br>        <span class="hljs-string">"labels"</span>:         labels,<br>    }<br>small_train = ds[<span class="hljs-string">"train"</span>].select(<span class="hljs-built_in">range</span>(<span class="hljs-number">10000</span>))<br><br>tokenized_train = small_train.<span class="hljs-built_in">map</span>(<br>    preprocess_fn,<br>    batched=<span class="hljs-literal">True</span>,<br>    num_proc=<span class="hljs-number">4</span>,<br>    remove_columns=[<span class="hljs-string">"id"</span>, <span class="hljs-string">"prompt"</span>, <span class="hljs-string">"response"</span>],<br>)<br></code></pre></td></tr></table></figure>

<blockquote>
<p>在 PyTorch／Transformers 里，nn.CrossEntropyLoss(ignore_index=-100) 会对 label 等于 -100 的位置跳过，不参与梯度计算和 loss 叠加。</p>
</blockquote>
<h2 id="Exciting-Training"><a href="#Exciting-Training" class="headerlink" title="Exciting Training"></a>Exciting Training</h2><p>这里我们训练的时候，使用了 <code>bf16</code> 混合精度训练，可以加速训练，并且减少显存占用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> (<br>    AutoModelForCausalLM,<br>    AutoTokenizer,<br>    Trainer,<br>    TrainingArguments,<br>)<br><br>training_args = TrainingArguments(<br>    output_dir=<span class="hljs-string">"./fine-tuned-model"</span>,<br>    per_device_train_batch_size=<span class="hljs-number">2</span>,<br>    gradient_accumulation_steps=<span class="hljs-number">4</span>,<br>    num_train_epochs=<span class="hljs-number">3</span>,<br>    learning_rate=<span class="hljs-number">2e-5</span>,<br>    bf16=<span class="hljs-literal">True</span>,<br>    logging_steps=<span class="hljs-number">100</span>,<br>    save_steps=<span class="hljs-number">500</span>,<br>    save_total_limit=<span class="hljs-number">3</span>,<br>)<br><br>trainer = Trainer(<br>    model=lora_model,<br>    args=training_args,<br>    train_dataset=tokenized_train,<br>)<br><br><span class="hljs-comment"># —— 5. 运行微调 —— </span><br>trainer.train()<br></code></pre></td></tr></table></figure>

<h2 id="Load-Saved-Model"><a href="#Load-Saved-Model" class="headerlink" title="Load Saved Model"></a>Load Saved Model</h2><p>模型保存下来后，需要加载模型时，需要先加载基础模型，然后再加载训练好的PEFT Adapter。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, AutoModelForCausalLM<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> PeftModel<br><span class="hljs-keyword">import</span> torch<br><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br>quantization_config = BitsAndBytesConfig(<br>    load_in_4bit=<span class="hljs-literal">True</span>,<br>    bnb_4bit_compute_dtype=torch.float16,<br>)<br><br>model_name = <span class="hljs-string">"Qwen/Qwen3-4B-Base"</span><br><br><span class="hljs-comment"># load the tokenizer and the model</span><br>tokenizer = AutoTokenizer.from_pretrained(model_name)<br>base_model = AutoModelForCausalLM.from_pretrained(<br>    model_name,<br>    torch_dtype=<span class="hljs-string">"auto"</span>,<br>    device_map=<span class="hljs-string">"auto"</span>,<br>    cache_dir=<span class="hljs-string">"/home/ec2-user/SageMaker/sagemaker"</span>,<br>    quantization_config=quantization_config<br>)<br><br><span class="hljs-comment"># 2) 加载 PEFT adapter</span><br>model = PeftModel.from_pretrained(<br>    base_model,<br>    <span class="hljs-string">"./fine-tuned-model"</span>       <span class="hljs-comment"># 这里是你 Trainer 输出的 output_dir</span><br>)<br>model.to(device)<br>model.<span class="hljs-built_in">eval</span>()<br><br><span class="hljs-comment"># 3) 加载 tokenizer</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">"./fine-tuned-model"</span>)<br><br><span class="hljs-comment"># —— 测试生成 —— </span><br><span class="hljs-comment"># —— 测试生成 —— </span><br>device = torch.device(<span class="hljs-string">"cuda"</span> <span class="hljs-keyword">if</span> torch.cuda.is_available() <span class="hljs-keyword">else</span> <span class="hljs-string">"cpu"</span>)<br><br>prompt = <span class="hljs-string">"Hello ?"</span><br>messages = [<br>    {<span class="hljs-string">"role"</span>: <span class="hljs-string">"user"</span>, <span class="hljs-string">"content"</span>: prompt},<br>]<br>text = tokenizer.apply_chat_template(<br>    messages,<br>    tokenize=<span class="hljs-literal">False</span>,<br>    add_generation_prompt=<span class="hljs-literal">True</span>,<br>    enable_thinking=<span class="hljs-literal">False</span><br>)<br>inputs = tokenizer(prompt, return_tensors=<span class="hljs-string">"pt"</span>).to(device)<br>out = model.generate(**inputs, max_new_tokens=<span class="hljs-number">1024</span>)<br><span class="hljs-built_in">print</span>(tokenizer.decode(out[<span class="hljs-number">0</span>], skip_special_tokens=<span class="hljs-literal">True</span>))<br></code></pre></td></tr></table></figure>

<h1 id="Appendix"><a href="#Appendix" class="headerlink" title="Appendix"></a>Appendix</h1><h2 id="两个测试GPU-Flops的example"><a href="#两个测试GPU-Flops的example" class="headerlink" title="两个测试GPU Flops的example"></a>两个测试GPU Flops的example</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> bitsandbytes.nn <span class="hljs-keyword">import</span> Linear8bitLt<br><span class="hljs-keyword">import</span> torch<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">benchmark_bnb_int8</span>(<span class="hljs-params"></span><br><span class="hljs-params">    size: <span class="hljs-built_in">int</span> = <span class="hljs-number">8192</span>,</span><br><span class="hljs-params">    warmup: <span class="hljs-built_in">int</span> = <span class="hljs-number">10</span>,</span><br><span class="hljs-params">    iters:  <span class="hljs-built_in">int</span> = <span class="hljs-number">200</span>,</span><br><span class="hljs-params"></span>):<br>    device = <span class="hljs-string">'cuda'</span><br>    <span class="hljs-comment"># 正确用法：input_features/output_features</span><br>    model = Linear8bitLt(<br>        input_features=size,<br>        output_features=size,<br>        bias=<span class="hljs-literal">False</span>,<br>        has_fp16_weights=<span class="hljs-literal">True</span>,    <span class="hljs-comment"># 混合 FP16 主权重</span><br>        threshold=<span class="hljs-number">6.0</span>,<br>        index=<span class="hljs-literal">False</span>,<br>        device=device<br>    )<br><br>    x = torch.randn(size, size, dtype=torch.float16, device=device)<br>    <span class="hljs-comment"># 预热</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(warmup):<br>        _ = model(x)<br>    torch.cuda.synchronize()<br><br>    <span class="hljs-comment"># 测时</span><br>    start = torch.cuda.Event(enable_timing=<span class="hljs-literal">True</span>)<br>    end   = torch.cuda.Event(enable_timing=<span class="hljs-literal">True</span>)<br>    start.record()<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters):<br>        _ = model(x)<br>    end.record()<br>    torch.cuda.synchronize()<br><br>    avg_ms = start.elapsed_time(end) / iters<br>    flops  = <span class="hljs-number">2</span> * size**<span class="hljs-number">3</span><br>    tflops = flops / (avg_ms/<span class="hljs-number">1e3</span>) / <span class="hljs-number">1e12</span><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"[bnb-INT8] Size=<span class="hljs-subst">{size}</span>, Avg Latency=<span class="hljs-subst">{avg_ms:<span class="hljs-number">.2</span>f}</span> ms → <span class="hljs-subst">{tflops:<span class="hljs-number">.2</span>f}</span> TFLOPS"</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>    benchmark_bnb_int8()<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> time<br><br>torch.backends.cuda.matmul.allow_tf32 = <span class="hljs-literal">True</span><br>torch.backends.cuda.matmul.allow_fp16_reduced_precision_reduction = <span class="hljs-literal">False</span><br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">benchmark_tflops</span>(<span class="hljs-params"></span><br><span class="hljs-params">    size: <span class="hljs-built_in">int</span> = <span class="hljs-number">8192</span>,</span><br><span class="hljs-params">    dtype: torch.dtype = torch.float32,</span><br><span class="hljs-params">    warmup_iters: <span class="hljs-built_in">int</span> = <span class="hljs-number">10</span>,</span><br><span class="hljs-params">    iters: <span class="hljs-built_in">int</span> = <span class="hljs-number">20</span>,</span><br><span class="hljs-params"></span>):<br>    <span class="hljs-string">"""</span><br><span class="hljs-string">    用 size x size 的矩阵做 iters 次乘法，并测量平均时长，计算 TFLOPS。</span><br><span class="hljs-string">    Args:</span><br><span class="hljs-string">      size: 矩阵维度 N（N x N）。</span><br><span class="hljs-string">      dtype: torch.float32 / float16 / bfloat16 等。</span><br><span class="hljs-string">      warmup_iters: 预热次数，丢弃前几次不稳定开销。</span><br><span class="hljs-string">      iters: 正式测量次数。</span><br><span class="hljs-string">    """</span><br>    device = <span class="hljs-string">'cuda'</span><br>    <span class="hljs-comment"># 随机矩阵</span><br>    A = torch.randn(size, size, device=device, dtype=dtype)<br>    B = torch.randn(size, size, device=device, dtype=dtype)<br><br>    <span class="hljs-comment"># 1. 预热</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(warmup_iters):<br>        _ = A @ B<br>    torch.cuda.synchronize()  <span class="hljs-comment"># 确保完成  [oai_citation_attribution:0‡Speechmatics](https://www.speechmatics.com/company/articles-and-news/timing-operations-in-pytorch?utm_source=chatgpt.com)</span><br><br>    <span class="hljs-comment"># 2. 创建 CUDA 事件</span><br>    start = torch.cuda.Event(enable_timing=<span class="hljs-literal">True</span>)<br>    end   = torch.cuda.Event(enable_timing=<span class="hljs-literal">True</span>)<br><br>    <span class="hljs-comment"># 3. 正式测时</span><br>    start.record()             <span class="hljs-comment"># 记录起点  [oai_citation_attribution:1‡PyTorch](https://pytorch.org/docs/stable/generated/torch.cuda.Event.html?utm_source=chatgpt.com)</span><br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(iters):<br>        C = A @ B<br>    end.record()               <span class="hljs-comment"># 记录终点</span><br>    torch.cuda.synchronize()   <span class="hljs-comment"># 等待所有核完成  [oai_citation_attribution:2‡PyTorch Forums](https://discuss.pytorch.org/t/how-to-measure-time-in-pytorch/26964/2?utm_source=chatgpt.com)</span><br><br>    <span class="hljs-comment"># 4. 计算平均耗时（ms）</span><br>    elapsed_ms = start.elapsed_time(end)<br>    avg_ms = elapsed_ms / iters<br><br>    <span class="hljs-comment"># 5. 计算 FLOPs:</span><br>    <span class="hljs-comment">#    每次矩阵乘法大约 2*N^3 次浮点运算 [oai_citation_attribution:3‡Reddit](https://www.reddit.com/r/compsci/comments/3141as/how_to_calculate_mflops_of_multiplying_square/?utm_source=chatgpt.com)</span><br>    flops_per_matmul = <span class="hljs-number">2</span> * size**<span class="hljs-number">3</span><br>    tflops = flops_per_matmul / (avg_ms / <span class="hljs-number">1e3</span>) / <span class="hljs-number">1e12</span><br><br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"矩阵维度: <span class="hljs-subst">{size}</span>×<span class="hljs-subst">{size}</span>, dtype=<span class="hljs-subst">{dtype}</span>"</span>)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">f"平均耗时: <span class="hljs-subst">{avg_ms:<span class="hljs-number">.3</span>f}</span> ms, 理论算力: <span class="hljs-subst">{tflops:<span class="hljs-number">.2</span>f}</span> TFLOPS"</span>)<br><br><span class="hljs-keyword">if</span> __name__ == <span class="hljs-string">"__main__"</span>:<br>    <span class="hljs-comment"># 测试不同数据类型</span><br>    <span class="hljs-keyword">for</span> dt <span class="hljs-keyword">in</span> [torch.float32, torch.float16, torch.bfloat16]:<br>        benchmark_tflops(size=<span class="hljs-number">8192</span>, dtype=dt)<br></code></pre></td></tr></table></figure>


        </div>
        
        <!-- 文章导航 -->
        <nav class="post-nav">
          
            <a class="prev" href="/2025/04/30/Peft-Finetuning-RL/">
              <i class="fas fa-chevron-left"></i>
              RLHF
            </a>
          
          
            <a class="next" href="/2025/04/20/Peft/">
              Parameter Efficient Fine-Tuning
              <i class="fas fa-chevron-right"></i>
            </a>
          
        </nav>
      </article>
    </div>
  </div>
</div>

    </div>
    <footer class="footer">
  <div class="status-bar">
    <div class="status-item">
      <i class="fas fa-code-branch"></i>
      master
    </div>
    <div class="status-item">
      <i class="fas fa-sync"></i>
      Tuntun
    </div>
    <div class="status-item">
      <i class="fas fa-clock"></i>
      2025-05-03
    </div>
    <div class="status-item">
      Designed By&nbsp; <a href="https://github.com/B143KC47" target="_blank"> BlackCat</a>
    </div>
    <div class="status-item github">
      <a href="#" target="_blank">
        <i class="fab fa-github"></i>
      </a>
    </div>
  </div>
</footer>

    
    <!-- 全局配置 -->
    <script>
      window.HEXO_CONFIG = {
        language: "en",
        root: "/"
      };
      
      // 特定于搜索的配置
      window.VSC4T_SEARCH = {
        root: "/"
      };
    </script>
    
    <script src="//cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
    <script src="//cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"></script>
    <script src="//cdn.jsdelivr.net/npm/highlight.js@11.7.0/highlight.min.js"></script>
    <script>hljs.highlightAll();</script>
    <!-- 这里可以放置自定义脚本 -->
<script>
document.addEventListener('DOMContentLoaded', (event) => {
  // Apply smooth scroll to non-TOC anchor links
  document.querySelectorAll('a[href^="#"]:not(.toc-link)').forEach(anchor => {
    anchor.addEventListener('click', function (e) {
      e.preventDefault();
      // Check if querySelector is valid before using it
      try {
        const targetSelector = this.getAttribute('href');
        // Basic check for potentially invalid selectors (though not exhaustive)
        if (targetSelector && targetSelector.length > 1) { 
          const targetElement = document.querySelector(targetSelector);
          if (targetElement) {
            targetElement.scrollIntoView({
              behavior: 'smooth'
            });
          } else {
            console.warn('Smooth scroll target not found:', targetSelector);
          }
        } else {
           console.warn('Invalid href for smooth scroll:', targetSelector);
        }
      } catch (error) {
        console.error('Error during smooth scroll:', error, 'Selector:', this.getAttribute('href'));
        // Fallback or alternative behavior if needed
        // For example, try getElementById if it's just an ID
        const targetId = this.getAttribute('href').slice(1);
        try {
            const targetElementById = document.getElementById(decodeURIComponent(targetId));
            if (targetElementById) {
                targetElementById.scrollIntoView({ behavior: 'smooth' });
            }
        } catch (idError) {
             console.error('Fallback getElementById also failed:', idError);
        }
      }
    });
  });
});
</script>
<script src="/js/toc.js"></script>

<!-- Scripts -->
<script>
  // 将语言文件中的翻译传递给前端
  window.HEXO_CONFIG = {
    language: "en",
    search_placeholder: "Type to search...",
    search_no_results: "No results found",
    search_result: "result",
    search_results: "results",
    search_results_found: "Found undefined results",
    search_in: "Search in",
    search_in_title: "Title",
    search_in_content: "Content",
    search_in_tags: "Tags",
    search_in_categories: "Categories",
    search_filters: "Search Filters",
    search_recent: "Recent Searches",
    search_clear: "Clear",
    search_loading: "Loading...",
    search_error: "Error loading search data"
  };
</script>



<!-- 添加所有需要的脚本 -->
<script src="/js/main.js"></script>
<script src="/js/search.js"></script>


    <script>
      // 移动端菜单切换
      $(document).ready(function() {
        $('.mobile-menu-toggle').click(function() {
          $('.sidebar-explorer').toggleClass('show');
        });
      });
    </script>
  </body>
</html>
